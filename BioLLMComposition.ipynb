{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LZn_zIc8FrEf"
      },
      "outputs": [],
      "source": [
        "#@title Load data (~4 min)\n",
        "!git clone https://github.com/jjoecclark/BioLLMComposition\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import random\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "from torch.optim import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "LR = 1e-3\n",
        "EPOCHS = 100\n",
        "VERBOSE = False\n",
        "\n",
        "# Load MHC data set\n",
        "MHCtrain = pd.read_csv('./BioLLMComposition/data/combo_1and2_train.tsv', sep='\\t')\n",
        "MHCval = pd.read_csv('./BioLLMComposition/data/combo_1and2_valid.tsv', sep='\\t')\n",
        "\n",
        "# Extract sequences from data frames\n",
        "train_seqs = MHCtrain['target_chainseq'].tolist()\n",
        "train_labs = MHCtrain['binder'].tolist()\n",
        "val_seqs = MHCval['target_chainseq'].tolist()\n",
        "val_labs = MHCval['binder'].tolist()\n",
        "\n",
        "# Preprocess sequences (protein and peptide are separated by '/')\n",
        "mhc_train_pep = []\n",
        "mhc_train_rec = []\n",
        "mhc_train_lab = []\n",
        "for i, s in enumerate(train_seqs):\n",
        "    try:\n",
        "        rec, pep = s.split('/')\n",
        "        mhc_train_pep.append(pep)\n",
        "        mhc_train_rec.append(rec)\n",
        "        mhc_train_lab.append(train_labs[i])\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "mhc_val_pep = []\n",
        "mhc_val_rec = []\n",
        "mhc_val_lab = []\n",
        "for i, s in enumerate(val_seqs):\n",
        "    try:\n",
        "        rec, pep = s.split('/')\n",
        "        mhc_val_pep.append(pep)\n",
        "        mhc_val_rec.append(rec)\n",
        "        mhc_val_lab.append(val_labs[i])\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "# Load pLM and PLM models\n",
        "esm_layers = 6\n",
        "esm_params = 8\n",
        "plm = AutoModelForMaskedLM.from_pretrained(f'facebook/esm2_t{esm_layers}_{esm_params}M_UR50D').to(device).eval()\n",
        "PLM = AutoModelForMaskedLM.from_pretrained(f'facebook/esm2_t{esm_layers}_{esm_params}M_UR50D').to(device).eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(f'facebook/esm2_t{esm_layers}_{esm_params}M_UR50D')\n",
        "\n",
        "# Get the mean embedding from esm style model\n",
        "def get_mean_rep(model_name, sequence):\n",
        "    token_ids = tokenizer(sequence, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        results = model_name.forward(token_ids.input_ids, output_hidden_states=True)\n",
        "    representations = results.hidden_states[-1][0]\n",
        "    mean_embedding = representations[1:len(sequence)+1].mean(dim=0)\n",
        "    return mean_embedding.cpu().numpy()\n",
        "\n",
        "# Custom data set class for peptide-protein pairs\n",
        "class mhcdataset(Dataset):\n",
        "    def __init__(self, peptides, proteins, labels, p_tokens, P_tokens):\n",
        "        self.peptides = peptides # Peptide sequenes\n",
        "        self.proteins = proteins # Protein sequences\n",
        "        self.labels = labels # Binary labels\n",
        "        self.p_tokens = p_tokens # ESM tokens for the sequences\n",
        "        self.P_tokens = P_tokens\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.labels.shape[0]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        p = self.peptides[i]\n",
        "        P = self.proteins[i]\n",
        "        y = self.labels[i]\n",
        "        p_tokens = {key: value[i] for key, value in self.p_tokens.items()}\n",
        "        P_tokens = {key: value[i] for key, value in self.P_tokens.items()}\n",
        "        return p, P, y, p_tokens, P_tokens\n",
        "\n",
        "# Extract training data set embeddings\n",
        "peptide_embs_train, protein_embs_train = [], []\n",
        "for pep, pro in tqdm(zip(mhc_train_pep, mhc_train_rec)):\n",
        "    peptide_embs_train.append(get_mean_rep(plm, pep))\n",
        "    protein_embs_train.append(get_mean_rep(PLM, pro))\n",
        "peptide_embs_train, protein_embs_train = np.array(peptide_embs_train), np.array(protein_embs_train)\n",
        "\n",
        "# Extract test data set embeddings\n",
        "peptide_embs_val, protein_embs_val = [], []\n",
        "for pep, pro in tqdm(zip(mhc_val_pep, mhc_val_rec)):\n",
        "    peptide_embs_val.append(get_mean_rep(plm, pep))\n",
        "    protein_embs_val.append(get_mean_rep(PLM, pro))\n",
        "peptide_embs_val, protein_embs_val = np.array(peptide_embs_val), np.array(protein_embs_val)\n",
        "\n",
        "# Tokenize\n",
        "pep_tokens_train = tokenizer(mhc_train_pep, return_tensors='pt', padding='max_length', max_length=9, truncation=True).to(device)\n",
        "pro_tokens_train = tokenizer(mhc_train_rec, return_tensors='pt', padding='max_length', max_length=181, truncation=True).to(device)\n",
        "pep_tokens_val = tokenizer(mhc_val_pep, return_tensors='pt', padding='max_length', max_length=9, truncation=True).to(device)\n",
        "pro_tokens_val = tokenizer(mhc_val_rec, return_tensors='pt', padding='max_length', max_length=181, truncation=True).to(device)\n",
        "\n",
        "# Make data sets\n",
        "train_data_set = mhcdataset(peptide_embs_train, protein_embs_train, np.array(mhc_train_lab), pep_tokens_train, pro_tokens_train)\n",
        "train_dataloader = DataLoader(train_data_set, batch_size=128, shuffle=True)\n",
        "test_data_set = mhcdataset(peptide_embs_val, protein_embs_val, np.array(mhc_val_lab), pep_tokens_val, pro_tokens_val)\n",
        "test_dataloader = DataLoader(test_data_set, batch_size=128, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train model on protein embeddings only\n",
        "\n",
        "# Train the model 3 independent times. Report the highest performance on the test set each time.\n",
        "for n in range(3):\n",
        "\n",
        "    # Simple model trained on concatenated peptide-protein embeddings\n",
        "    model = torch.nn.Sequential(\n",
        "        torch.nn.Linear(320, 128),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(128, 2),\n",
        "        torch.nn.Softmax(dim=1)\n",
        "    )\n",
        "\n",
        "    # Move to device, define optimizer, loss\n",
        "    model = model.to(device)\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Record the best test set accuracy\n",
        "    best_test_acc = 0\n",
        "\n",
        "    # 500 training epochs per run\n",
        "    for epoch in tqdm(range(EPOCHS)):\n",
        "\n",
        "        # Start training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Load data for batch\n",
        "        for batch in train_dataloader:\n",
        "            _, protein_embs, labels, _, _ = batch\n",
        "\n",
        "            # perform forward pass, compute loss\n",
        "            protein_embs, labels = protein_embs.to(device), labels.to(device)\n",
        "            outputs = model(protein_embs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "            # Get classification performance\n",
        "            train_loss += loss.item() * labels.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "        train_loss /= total\n",
        "        train_accuracy = correct / total\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in test_dataloader:\n",
        "                _, protein_embs, labels, _, _ = batch\n",
        "\n",
        "                # Concatenate, forward pass, compute loss\n",
        "                protein_embs, labels = protein_embs.to(device), labels.to(device)\n",
        "                outputs = model(protein_embs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                test_loss += loss.item() * labels.size(0)\n",
        "\n",
        "                # Get classification performance\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "            test_loss /= total\n",
        "            test_accuracy = correct / total\n",
        "\n",
        "            # Save if model had best accuracy so far\n",
        "            if test_accuracy > best_test_acc:\n",
        "                best_test_acc = test_accuracy\n",
        "                torch.save(model.state_dict(), \"protein_emb_model.pth\")\n",
        "            if VERBOSE: print(f\"Epoch {epoch + 1}, Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    # Print the best performance over all 500 epochs\n",
        "    print(best_test_acc)\n",
        "\n",
        "# Visualize protein embeddings\n",
        "embs_cat = protein_embs_val\n",
        "\n",
        "# Perform PCA\n",
        "embs_cat = np.array(embs_cat)\n",
        "tsne = TSNE(n_components=2, perplexity=10, random_state=1)\n",
        "embs_cat = tsne.fit_transform(embs_cat)\n",
        "\n",
        "# Colormap\n",
        "cm = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"royalblue\",\"mediumseagreen\",\"crimson\"])\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(6, 6))\n",
        "scatter = plt.scatter(embs_cat[:, 0], embs_cat[:, 1], c=[1]*embs_cat.shape[0], cmap=cm, s=10, alpha=0.9)\n",
        "plt.xlabel(\"t-SNE-1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE-2\", fontsize=18)\n",
        "plt.scatter([], [], c='red', edgecolor='k', alpha=0.7, label='Binding pair')\n",
        "plt.scatter([], [], c='blue', edgecolor='k', alpha=0.7, label='Non-binding pair')\n",
        "plt.tick_params(axis='both', which='major', labelsize=18)\n",
        "plt.savefig('./protein_only.png', dpi=600, bbox_inches='tight')\n",
        "plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "o9WsYbM-bdbx",
        "outputId": "61df80f8-3e43-4033-8c4b-efc19ea8fa78"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:44<00:00,  2.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5194610778443114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:43<00:00,  2.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5209580838323353\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:43<00:00,  2.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5169660678642715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train model on peptide embeddings only\n",
        "\n",
        "# Train the model 3 independent times. Report the highest performance on the test set each time.\n",
        "for n in range(3):\n",
        "\n",
        "    # Simple model trained on concatenated peptide-protein embeddings\n",
        "    model = torch.nn.Sequential(\n",
        "        torch.nn.Linear(320, 128),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(128, 2),\n",
        "        torch.nn.Softmax(dim=1)\n",
        "    )\n",
        "\n",
        "    # Move to device, define optimizer, loss\n",
        "    model = model.to(device)\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Record the best test set accuracy\n",
        "    best_test_acc = 0\n",
        "\n",
        "    # 500 training epochs per run\n",
        "    for epoch in tqdm(range(EPOCHS)):\n",
        "\n",
        "        # Start training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Load data for batch\n",
        "        for batch in train_dataloader:\n",
        "            peptide_embs, _, labels, _, _ = batch\n",
        "\n",
        "            # perform forward pass, compute loss\n",
        "            peptide_embs, labels = peptide_embs.to(device), labels.to(device)\n",
        "            outputs = model(peptide_embs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "            # Get classification performance\n",
        "            train_loss += loss.item() * labels.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "        train_loss /= total\n",
        "        train_accuracy = correct / total\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in test_dataloader:\n",
        "                peptide_embs, _, labels, _, _ = batch\n",
        "\n",
        "                # forward pass, compute loss\n",
        "                peptide_embs, labels = peptide_embs.to(device), labels.to(device)\n",
        "                outputs = model(peptide_embs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                test_loss += loss.item() * labels.size(0)\n",
        "\n",
        "                # Get classification performance\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "            test_loss /= total\n",
        "            test_accuracy = correct / total\n",
        "\n",
        "            # Save if model had best accuracy so far\n",
        "            if test_accuracy > best_test_acc:\n",
        "                best_test_acc = test_accuracy\n",
        "                torch.save(model.state_dict(), \"peptide_emb_model.pth\")\n",
        "            if VERBOSE: print(f\"Epoch {epoch + 1}, Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    # Print the best performance over all 500 epochs\n",
        "    print(best_test_acc)\n",
        "\n",
        "# Visualize peptide embeddings\n",
        "embs_cat = peptide_embs_val\n",
        "\n",
        "# Perform PCA\n",
        "embs_cat = np.array(embs_cat)\n",
        "tsne = TSNE(n_components=2, perplexity=10, random_state=1)\n",
        "embs_cat = tsne.fit_transform(embs_cat)\n",
        "\n",
        "# Colormap\n",
        "cm = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"royalblue\",\"mediumseagreen\",\"crimson\"])\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(6, 6))\n",
        "scatter = plt.scatter(embs_cat[:, 0], embs_cat[:, 1], c=[1]*embs_cat.shape[0], cmap=cm, s=10, alpha=0.9)\n",
        "plt.xlabel(\"t-SNE-1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE-2\", fontsize=18)\n",
        "plt.scatter([], [], c='red', edgecolor='k', alpha=0.7, label='Binding pair')\n",
        "plt.scatter([], [], c='blue', edgecolor='k', alpha=0.7, label='Non-binding pair')\n",
        "plt.tick_params(axis='both', which='major', labelsize=18)\n",
        "plt.savefig('./peptide_only.png', dpi=600, bbox_inches='tight')\n",
        "plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "X46kebTrF2GE",
        "outputId": "83718551-4c53-4cfb-f06d-bf663301f590"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:43<00:00,  2.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7529940119760479\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:43<00:00,  2.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7554890219560878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:43<00:00,  2.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7634730538922155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train model on concatenated embeddings\n",
        "# Train the model 3 independent times. Report the highest performance on the test set each time.\n",
        "for n in range(3):\n",
        "\n",
        "    # Simple model trained on concatenated peptide-protein embeddings\n",
        "    model = torch.nn.Sequential(\n",
        "        torch.nn.Linear(640, 128),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(128, 2),\n",
        "        torch.nn.Softmax(dim=1)\n",
        "    )\n",
        "\n",
        "    # Move to device, define optimizer, loss\n",
        "    model = model.to(device)\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Record the best test set accuracy\n",
        "    best_test_acc = 0\n",
        "\n",
        "    # 500 training epochs per run\n",
        "    for epoch in tqdm(range(EPOCHS)):\n",
        "\n",
        "        # Start training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Load data for batch\n",
        "        for batch in train_dataloader:\n",
        "            peptide_embs, protein_embs, labels, _, _ = batch\n",
        "\n",
        "            # Concatenation occurs here\n",
        "            inputs = torch.cat((peptide_embs, protein_embs), dim=1)\n",
        "\n",
        "            # perform forward pass, compute loss\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "            # Get classification performance\n",
        "            train_loss += loss.item() * labels.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "        train_loss /= total\n",
        "        train_accuracy = correct / total\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in test_dataloader:\n",
        "                peptide_embs, protein_embs, labels, _, _ = batch\n",
        "\n",
        "                # Concatenate, forward pass, compute loss\n",
        "                inputs = torch.cat((peptide_embs, protein_embs), dim=1)\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                test_loss += loss.item() * labels.size(0)\n",
        "\n",
        "                # Get classification performance\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "            test_loss /= total\n",
        "            test_accuracy = correct / total\n",
        "\n",
        "            # Save if model had best accuracy so far\n",
        "            if test_accuracy > best_test_acc:\n",
        "                best_test_acc = test_accuracy\n",
        "                torch.save(model.state_dict(), \"concat_model.pth\")\n",
        "            if VERBOSE: print(f\"Epoch {epoch + 1}, Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    # Print the best performance over all 500 epochs\n",
        "    print(best_test_acc)\n",
        "\n",
        "# Visualization for concatenation\n",
        "\n",
        "# Concatenate\n",
        "embs_cat = np.concatenate((peptide_embs_val, protein_embs_val), axis=1)\n",
        "\n",
        "# Perform PCA\n",
        "embs_cat = np.array(embs_cat)\n",
        "tsne = TSNE(n_components=2, perplexity=10, random_state=1)\n",
        "embs_cat = tsne.fit_transform(embs_cat)\n",
        "\n",
        "# Colormap\n",
        "cm = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"royalblue\",\"mediumseagreen\",\"crimson\"])\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(6, 6))\n",
        "scatter = plt.scatter(embs_cat[:, 0], embs_cat[:, 1], c=mhc_val_lab, cmap=cm, s=10, alpha=0.9)\n",
        "plt.xlabel(\"t-SNE-1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE-2\", fontsize=18)\n",
        "plt.scatter([], [], c='red', edgecolor='k', alpha=0.7, label='Binding pair')\n",
        "plt.scatter([], [], c='blue', edgecolor='k', alpha=0.7, label='Non-binding pair')\n",
        "plt.tick_params(axis='both', which='major', labelsize=18)\n",
        "plt.savefig('./concatenation.png', dpi=600, bbox_inches='tight')\n",
        "plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "0bRC3esMcmi9",
        "outputId": "d801da1d-66e1-49a3-aa9e-b699f195ee3d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:46<00:00,  2.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.813872255489022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:46<00:00,  2.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8348303393213573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:46<00:00,  2.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8153692614770459\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train contrastive learning model\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import CosineSimilarity\n",
        "\n",
        "# Simple contrastive learning model\n",
        "class CLModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CLModel, self).__init__()\n",
        "\n",
        "    # Projection network for peptide\n",
        "    self.peptide_proj = torch.nn.Sequential(\n",
        "      torch.nn.Linear(320, 128),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.Linear(128, 128),\n",
        "    )\n",
        "\n",
        "    # Projection network for protein\n",
        "    self.protein_proj = torch.nn.Sequential(\n",
        "      torch.nn.Linear(320, 128),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.Linear(128, 128),\n",
        "    )\n",
        "\n",
        "  def forward(self, pep, pro):\n",
        "    # Project and normalize\n",
        "    pep = self.peptide_proj(pep)\n",
        "    pro = self.protein_proj(pro)\n",
        "    pep = F.normalize(pep, p=2, dim=1)\n",
        "    pro = F.normalize(pro, p=2, dim=1)\n",
        "    return pep, pro\n",
        "\n",
        "# Train the model 3 independent times. Report the highest performance on the test set each time.\n",
        "for n in range(3):\n",
        "\n",
        "    # Define model and other components\n",
        "    model = CLModel()\n",
        "    model = model.to(device)\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "    criterion = nn.CosineEmbeddingLoss()\n",
        "\n",
        "    best_test_acc = 0\n",
        "    for epoch in tqdm(range(EPOCHS)):\n",
        "\n",
        "        # Start training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Load data for batch\n",
        "        for batch in train_dataloader:\n",
        "            peptide_embs, protein_embs, labels, _, _ = batch\n",
        "\n",
        "            # Replace zeros with -1s for cosine loss\n",
        "            labels[labels == 0] = -1\n",
        "\n",
        "            # Forward pass\n",
        "            peptide_embs, protein_embs, labels = peptide_embs.to(device), protein_embs.to(device), labels.to(device)\n",
        "            pep_out, pro_out = model(peptide_embs, protein_embs)\n",
        "\n",
        "            # Cosine similarity loss\n",
        "            loss = criterion(pep_out, pro_out, labels)\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "            # Get classification metrics via cosine similarity\n",
        "            train_loss += loss.item() * labels.size(0)\n",
        "            similarities = CosineSimilarity(dim=1, eps=1e-6)(pep_out, pro_out)\n",
        "            preds = (similarities > 0).long() * 2 - 1  # Map to -1 or 1\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "        train_loss /= total\n",
        "        train_accuracy = correct / total\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in test_dataloader:\n",
        "                peptide_embs, protein_embs, labels, _, _ = batch\n",
        "\n",
        "                # Replace zeros with -1s for cosine loss\n",
        "                labels[labels == 0] = -1\n",
        "\n",
        "                # Forward pass, Cosine similarity loss\n",
        "                peptide_embs, protein_embs, labels = peptide_embs.to(device), protein_embs.to(device), labels.to(device)\n",
        "                pep_out, pro_out = model(peptide_embs, protein_embs)\n",
        "                loss = criterion(pep_out, pro_out, labels)\n",
        "\n",
        "                # Get classification metrics by cosine sim\n",
        "                test_loss += loss.item() * labels.size(0)\n",
        "                similarities = CosineSimilarity(dim=1, eps=1e-6)(pep_out, pro_out)\n",
        "                preds = (similarities > 0).long() * 2 - 1 # Map to -1 or 1\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "            test_loss /= total\n",
        "            test_accuracy = correct / total\n",
        "\n",
        "            # Save if model had best accuracy so far\n",
        "            if test_accuracy > best_test_acc:\n",
        "                best_test_acc = test_accuracy\n",
        "                torch.save(model.state_dict(), \"contrastive_model.pth\")\n",
        "            if VERBOSE: print(f\"Epoch {epoch + 1}, Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    print(best_test_acc)\n",
        "\n",
        "# Visualization for contrastive learning\n",
        "\n",
        "model = CLModel()\n",
        "model = model.to(device)\n",
        "model.load_state_dict(torch.load(\"./contrastive_model.pth\", weights_only=True))\n",
        "model.eval()\n",
        "\n",
        "embs_contrastive = []\n",
        "y_true = []\n",
        "with torch.no_grad():\n",
        "  for batch in test_dataloader:\n",
        "    peptide_embs, protein_embs, labels, _, _ = batch\n",
        "    peptide_embs, protein_embs, labels = peptide_embs.to(device), protein_embs.to(device), labels.to(device)\n",
        "    pep_out, pro_out = model(peptide_embs, protein_embs)\n",
        "\n",
        "    for example, y in zip(pep_out, labels):\n",
        "      embs_contrastive.append(example.detach().cpu().numpy())\n",
        "      y_true.append(1)\n",
        "\n",
        "    for example in pro_out:\n",
        "      embs_contrastive.append(example.detach().cpu().numpy())\n",
        "      y_true.append(0.5)\n",
        "\n",
        "# Perform PCA\n",
        "embs_contrastive = np.array(embs_contrastive)\n",
        "tsne = TSNE(n_components=2, perplexity=10, random_state=1)\n",
        "embs_contrastive = tsne.fit_transform(embs_contrastive)\n",
        "\n",
        "# Colormap\n",
        "cm = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"darkorange\",\"royalblue\"])\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(6, 6))\n",
        "scatter = plt.scatter(embs_contrastive[:, 0], embs_contrastive[:, 1], c=y_true, cmap=cm, s=10, alpha=0.9)\n",
        "plt.xlabel(\"t-SNE-1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE-2\", fontsize=18)\n",
        "plt.scatter([], [], c='crimson', edgecolor='k', alpha=0.7, label='Binding pair')\n",
        "plt.scatter([], [], c='royalblue', edgecolor='k', alpha=0.7, label='Non-binding pair')\n",
        "plt.scatter([], [], c='mediumseagreen', edgecolor='k', alpha=0.7, label='Non-binding pair')\n",
        "plt.tick_params(axis='both', which='major', labelsize=18)\n",
        "plt.savefig('./contrastive.png', dpi=600, bbox_inches='tight')\n",
        "plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "fCuMrJfdc8NS",
        "outputId": "480c8ef0-e4d8-4335-9bdd-b92ee88c59f7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:02<00:00,  1.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.810379241516966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:24<00:00,  1.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8033932135728543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:11<00:00,  1.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8033932135728543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train attention model\n",
        "EPOCHS=100\n",
        "# Minimal cross attention network\n",
        "class AttentionModel(nn.Module):\n",
        "  def __init__(self, plm, PLM):\n",
        "    super(AttentionModel, self).__init__()\n",
        "    self.plm = plm\n",
        "    self.PLM = PLM\n",
        "    self.attn = nn.MultiheadAttention(320, 1, batch_first=True)\n",
        "    self.prediction_head = torch.nn.Sequential(\n",
        "        torch.nn.Linear(320, 2),\n",
        "        # torch.nn.ReLU(),\n",
        "        # torch.nn.Linear(128, 2),\n",
        "        torch.nn.Softmax(dim=1)\n",
        "    )\n",
        "    self.post_attn_norm = nn.LayerNorm(320)\n",
        "    # Freeze the anchor and augment models\n",
        "    for param in self.plm.parameters(): param.requires_grad = False\n",
        "    for param in self.PLM.parameters(): param.requires_grad = False\n",
        "\n",
        "  def forward(self, pep_tokens, pro_tokens, attn_mask=None):\n",
        "\n",
        "    # Obtain protein sequence embedding L x D. len(pro_tokens.input_ids) is len(protein_sequence)+2\n",
        "    with torch.no_grad():\n",
        "        results = self.PLM.forward(pro_tokens['input_ids'], pro_tokens['attention_mask'], output_hidden_states=True)\n",
        "    t1 = results.hidden_states[-1]\n",
        "\n",
        "    # Obtain peptide sequence embedding L x D. len(pep_tokens.input_ids) is len(peptide_sequence)+2\n",
        "    with torch.no_grad():\n",
        "        results = self.plm.forward(pep_tokens['input_ids'], pep_tokens['attention_mask'], output_hidden_states=True)\n",
        "    t2 = results.hidden_states[-1]\n",
        "\n",
        "    # Create attn mask\n",
        "    attn_mask = torch.matmul(\n",
        "        pro_tokens[\"attention_mask\"].unsqueeze(2).float(),\n",
        "        pep_tokens['attention_mask'].unsqueeze(1).float(),\n",
        "    ).repeat(1, 1, 1)\n",
        "\n",
        "    # Perform attention\n",
        "    output, attn_weights = self.attn(\n",
        "        query=t1, key=t2, value=t2, attn_mask=attn_mask, average_attn_weights=False\n",
        "    )\n",
        "    output = self.post_attn_norm(output) + t1\n",
        "\n",
        "    # Take the mean. Exclude padding tokens and bos/eos\n",
        "    mask_sum = pro_tokens['attention_mask'].sum(dim=1, keepdim=True).clamp(min=1e-6)  # Avoid division by zero\n",
        "    output = (output * pro_tokens['attention_mask'].unsqueeze(2)).sum(dim=1) / mask_sum\n",
        "\n",
        "    return self.prediction_head(output), output\n",
        "\n",
        "for n in range(1):\n",
        "    # Define model and other components\n",
        "    model = AttentionModel(plm, PLM)\n",
        "    model = model.to(device)\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    best_test_acc = 0\n",
        "    for epoch in tqdm(range(EPOCHS)):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch in train_dataloader:\n",
        "            # Forward pass\n",
        "            _, _, labels, pep_tokens, pro_tokens = batch\n",
        "            labels = labels.to(device)\n",
        "            outputs, _ = model(pep_tokens, pro_tokens)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "            # Compute classification metrics\n",
        "            train_loss += loss.item() * labels.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "        train_loss /= total\n",
        "        train_accuracy = correct / total\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in test_dataloader:\n",
        "\n",
        "                # Forward pass\n",
        "                _, _, labels, pep_tokens, pro_tokens = batch\n",
        "                labels = labels.to(device)\n",
        "                outputs, _ = model(pep_tokens, pro_tokens)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Compute classification metrics\n",
        "                test_loss += loss.item() * labels.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "            test_loss /= total\n",
        "            test_accuracy = correct / total\n",
        "\n",
        "        # Save if best\n",
        "        if test_accuracy > best_test_acc:\n",
        "            best_test_acc = test_accuracy\n",
        "            torch.save(model.state_dict(), \"attention_model.pth\")\n",
        "        if VERBOSE: print(f\"Epoch {epoch + 1}, Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(best_test_acc)\n",
        "\n",
        "#@title Visualization for attention\n",
        "model = AttentionModel(plm, PLM)\n",
        "model = model.to(device)\n",
        "model.load_state_dict(torch.load(\"./attention_model.pth\", weights_only=True))\n",
        "model.eval()\n",
        "\n",
        "embs_attention = []\n",
        "y_true = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        _, _, labels, pep_tokens, pro_tokens = batch\n",
        "        labels = labels.to(device)\n",
        "        pep_out, _ = model(pep_tokens, pro_tokens)\n",
        "        for example, y in zip(pep_out, labels):\n",
        "            embs_attention.append(example.detach().cpu().numpy())\n",
        "            y_true.append(y.item())\n",
        "embs_attention = np.array(embs_attention)\n",
        "\n",
        "# Perform PCA\n",
        "embs_attention = np.array(embs_attention)\n",
        "tsne = TSNE(n_components=2, perplexity=10, random_state=1)\n",
        "embs_attention = tsne.fit_transform(embs_attention)\n",
        "\n",
        "# Colormap\n",
        "cm = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"royalblue\",\"crimson\"])\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(6, 6))\n",
        "scatter = plt.scatter(embs_attention[:, 0], embs_attention[:, 1], c=y_true, cmap=cm, s=10, alpha=0.9)\n",
        "plt.xlabel(\"t-SNE-1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE-2\", fontsize=18)\n",
        "plt.scatter([], [], c='crimson', edgecolor='k', alpha=0.7, label='Binding pair')\n",
        "plt.scatter([], [], c='royalblue', edgecolor='k', alpha=0.7, label='Non-binding pair')\n",
        "plt.scatter([], [], c='mediumseagreen', edgecolor='k', alpha=0.7, label='Non-binding pair')\n",
        "plt.tick_params(axis='both', which='major', labelsize=18)\n",
        "plt.savefig('./attention.png', dpi=600, bbox_inches='tight')\n",
        "plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "ddf5923d-6cf7-4767-cad9-684012b052d3",
        "id": "h6VYFqxJd0sh"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [37:07<00:00, 22.27s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8363273453093812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train composition style model\n",
        "\n",
        "# Minimal composition of language models\n",
        "target_layers = [0, 3, 5]\n",
        "class CompositionModel(nn.Module):\n",
        "  def __init__(self, plm, PLM):\n",
        "    super(CompositionModel, self).__init__()\n",
        "    self.plm = plm\n",
        "    self.PLM = PLM\n",
        "    self.cross_atten_layers = nn.ModuleList([\n",
        "        nn.MultiheadAttention(320, 20, batch_first=True) for i in range(len(target_layers))\n",
        "    ])\n",
        "    self.post_attn_norms = nn.ModuleList([\n",
        "        nn.LayerNorm(320) for i in range(len(target_layers))\n",
        "    ])\n",
        "    self.prediction_head = torch.nn.Sequential(\n",
        "        torch.nn.Linear(320, 128),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(128, 2),\n",
        "        torch.nn.Softmax(dim=1)\n",
        "    )\n",
        "    self.esm_layers = 6\n",
        "    # Freeze the anchor and augment models\n",
        "    for param in self.plm.parameters(): param.requires_grad = False\n",
        "    for param in self.PLM.parameters(): param.requires_grad = False\n",
        "\n",
        "  def forward(self, pep_input, prot_input, attn_mask=None):\n",
        "    # Create attn mask\n",
        "    attn_mask = torch.matmul(\n",
        "        prot_input[\"attention_mask\"].unsqueeze(2).float(),\n",
        "        pep_input['attention_mask'].unsqueeze(1).float(),\n",
        "    ).repeat(20, 1, 1)\n",
        "\n",
        "    # Create the esm attention masks correctly\n",
        "    pep_attn_mask = self.plm.get_extended_attention_mask(pep_input['attention_mask'], pep_input[\"input_ids\"].size())\n",
        "    prot_attn_mask = self.PLM.get_extended_attention_mask(prot_input['attention_mask'], prot_input[\"input_ids\"].size())\n",
        "\n",
        "    # Embedding layer\n",
        "    pro = self.PLM.esm.embeddings(prot_input[\"input_ids\"], prot_input[\"attention_mask\"]).to(device)\n",
        "    with torch.no_grad():\n",
        "        pep = self.plm.esm.embeddings(pep_input[\"input_ids\"], pep_input[\"attention_mask\"]).to(device)\n",
        "\n",
        "    # Layerwise forward pass\n",
        "    counter = 0\n",
        "    for i in range(0, self.esm_layers):\n",
        "\n",
        "        # Update embeddings\n",
        "        pro = self.PLM.esm.encoder.layer[i](pro, prot_attn_mask)[0]#, prot_input[\"attention_mask\"][:, None, None, :])[0]\n",
        "        with torch.no_grad():\n",
        "            pep = self.plm.esm.encoder.layer[i](pep, pep_attn_mask)[0]#, pep_input[\"attention_mask\"][:, None, None, :])[0]\n",
        "\n",
        "        if i in target_layers:\n",
        "            # Perform cross attn and layer norm\n",
        "            attn_out, _ = self.cross_atten_layers[counter](query=pro, key=pep, value=pep, attn_mask=attn_mask, average_attn_weights=False)\n",
        "            attn_out = self.post_attn_norms[counter](attn_out)\n",
        "            pro = pro + attn_out\n",
        "            counter += 1\n",
        "\n",
        "    pro = self.PLM.esm.encoder.emb_layer_norm_after(pro)\n",
        "\n",
        "    # Take the mean. Exclude padding tokens and bos/eos\n",
        "    mask_sum = prot_input['attention_mask'].sum(dim=1, keepdim=True).clamp(min=1e-6)  # Avoid division by zero\n",
        "    pro = (pro * prot_input['attention_mask'].unsqueeze(2)).sum(dim=1) / mask_sum\n",
        "    return self.prediction_head(pro), pro\n",
        "\n",
        "for n in range(1):\n",
        "\n",
        "    # Define model and other components\n",
        "    model = CompositionModel(plm, PLM)\n",
        "    model = model.to(device)\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    best_test_acc = 0\n",
        "    for epoch in tqdm(range(EPOCHS)):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch in train_dataloader:\n",
        "            # Forward pass\n",
        "            _, _, labels, pep_tokens, pro_tokens = batch\n",
        "            labels = labels.to(device)\n",
        "            outputs, _ = model(pep_tokens, pro_tokens)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "            # Compute classification metrics\n",
        "            train_loss += loss.item() * labels.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "        train_loss /= total\n",
        "        train_accuracy = correct / total\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in test_dataloader:\n",
        "                # Forward pass\n",
        "                _, _, labels, pep_tokens, pro_tokens = batch\n",
        "                labels = labels.to(device)\n",
        "                outputs, _ = model(pep_tokens, pro_tokens)\n",
        "                loss = criterion(outputs, labels)\n",
        "                # Compute classification metrics\n",
        "                test_loss += loss.item() * labels.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "        test_loss /= total\n",
        "        test_accuracy = correct / total\n",
        "\n",
        "        if test_accuracy > best_test_acc:\n",
        "            best_test_acc = test_accuracy\n",
        "            #torch.save(model.state_dict(), \"comp_model.pth\")\n",
        "        if VERBOSE: print(f\"Epoch {epoch + 1}, Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    print(best_test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "n7cxREigfp9U",
        "outputId": "e2322330-fbf6-43d6-e731-0b964f1afe91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [1:06:16<00:00, 39.76s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8388223552894212\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        }
      ]
    }
  ]
}